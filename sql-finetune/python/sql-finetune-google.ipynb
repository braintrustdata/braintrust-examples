{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCP8uX0pyXMD"
   },
   "source": [
    "# BrainTrust Text2SQL Fine Tune Tutorial\n",
    "\n",
    "Welcome to [BrainTrust](https://www.braintrustdata.com/)! This tutorial will teach you how to finetune a `gpt-3.5-turbo` to generate SQL and evaluate it using BrainTrust compared to regular `gpt-3.5-turbo`.\n",
    "\n",
    "Before starting, please make sure that you _already_ have a BrainTrust account. If you do not, please [sign up](https://www.braintrustdata.com) or [get in touch](mailto:info@braintrustdata.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTYUb2b9yXMF"
   },
   "source": [
    "## 1. Install and setup variables\n",
    "Let's first setup our API key variables and install some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Replace YOUR_OPENAI_KEY with your OpenAI API Key and YOUR_BRAINTRUST_API_KEY with your BrainTrust API key. Do not put it in quotes.\n",
    "%env OPENAI_API_KEY=\n",
    "%env GOOGLE_AI_API_KEY=\n",
    "%env BRAINTRUST_API_KEY="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tyu2HpZTzp9O"
   },
   "outputs": [],
   "source": [
    "%pip install -U /home/ubuntu/braintrust/braintrust/sdk/py duckdb datasets openai pyarrow autoevals google-generativeai\n",
    "%pip install -U google-auth google-auth-oauthlib google-auth-httplib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kti5t70nyXMF"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import braintrust\n",
    "import google.generativeai as palm\n",
    "import os\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Pt7PHoryXMF"
   },
   "source": [
    "## 2. Define helper functions\n",
    "We'll define some helper functions that help us work with SQL related data and queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ib_d5Dh7yXMF"
   },
   "outputs": [],
   "source": [
    "# Import libraries + define helper functions\n",
    "import duckdb\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from Levenshtein import distance\n",
    "import openai\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import time\n",
    "\n",
    "NUM_TEST_EXAMPLES = 30\n",
    "\n",
    "# Define some helper functions\n",
    "def get_table(table):\n",
    "    rows = [\n",
    "        {h: row[i] for (i, h) in enumerate(table[\"header\"])} for row in table[\"rows\"]\n",
    "    ]\n",
    "\n",
    "    return pa.Table.from_pylist(rows)\n",
    "\n",
    "AGG_OPS = [None, \"MAX\", \"MIN\", \"COUNT\", \"SUM\", \"AVG\"]\n",
    "COND_OPS = [\" ILIKE \", \">\", \"<\"]  # , \"OP\"]\n",
    "\n",
    "\n",
    "def esc_fn(s):\n",
    "    return f'''\"{s.replace('\"', '\"\"')}\"'''\n",
    "\n",
    "\n",
    "def esc_value(s):\n",
    "    if isinstance(s, str):\n",
    "        return s.replace(\"'\", \"''\")\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "def codegen_query(query):\n",
    "    header = query[\"table\"][\"header\"]\n",
    "\n",
    "    projection = f\"{esc_fn(header[query['sql']['sel']])}\"\n",
    "\n",
    "    agg_op = AGG_OPS[query[\"sql\"][\"agg\"]]\n",
    "    if agg_op is not None:\n",
    "        projection = f\"{agg_op}({projection})\"\n",
    "\n",
    "    conds = query[\"sql\"][\"conds\"]\n",
    "\n",
    "    filters = \" and \".join(\n",
    "        [\n",
    "            f\"\"\"{esc_fn(header[field])}{COND_OPS[cond]}'{esc_value(value)}'\"\"\"\n",
    "            for (field, cond, value) in zip(\n",
    "                conds[\"column_index\"], conds[\"operator_index\"], conds[\"condition\"]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if filters:\n",
    "        filters = f\" WHERE {filters}\"\n",
    "\n",
    "    return f'SELECT {projection} FROM \"table\"{filters}'\n",
    "\n",
    "OPENAI_CACHE = None\n",
    "def openai_req(model, messages, max_tokens):\n",
    "    global OPENAI_CACHE\n",
    "    if OPENAI_CACHE is None:\n",
    "        os.makedirs(\"data\", exist_ok=True)\n",
    "        OPENAI_CACHE = duckdb.connect(database=\"data/oai_cache.duckdb\")\n",
    "        OPENAI_CACHE.query(\n",
    "            \"CREATE TABLE IF NOT EXISTS cache (params text, response text)\"\n",
    "        )\n",
    "\n",
    "    for i in range(5):\n",
    "      try:\n",
    "        resp = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        break\n",
    "      except openai.error.RateLimitError:\n",
    "        print(\"Rate limited... Sleeping for 30 seconds\")\n",
    "        time.sleep(30)\n",
    "\n",
    "    response_text = resp[\"choices\"][0]['message'][\"content\"]\n",
    "\n",
    "    # messages to string\n",
    "    prompt = \"\".join([m[\"content\"] for m in messages])\n",
    "\n",
    "    return prompt, response_text\n",
    "\n",
    "def green(s):\n",
    "  return \"\\x1b[32m\" + s + \"\\x1b[0m\"\n",
    "\n",
    "def run_query(sql, table_record):\n",
    "    table = get_table(table_record)  # noqa\n",
    "    rel_from_arrow = duckdb.arrow(table)\n",
    "\n",
    "    result = rel_from_arrow.query(\"table\", sql).fetchone()\n",
    "    if result and len(result) > 0:\n",
    "        return result[0]\n",
    "    return None\n",
    "\n",
    "def score(r1, r2):\n",
    "    if r1 is None and r2 is None:\n",
    "        return 1\n",
    "    if r1 is None or r2 is None:\n",
    "        return 0\n",
    "\n",
    "    r1, r2 = str(r1), str(r2)\n",
    "\n",
    "    total_len = max(len(r1), len(r2))\n",
    "    return 1 - distance(r1, r2) / total_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGf_3bcqyXMG"
   },
   "source": [
    "3. ## Prepare a training set\n",
    "\n",
    "We'll use the `wikisql`[ dataset from Hugging Face](https://huggingface.co/datasets/wikisql) to create a training set of data to fine tune a model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEd3nKbbyXMG"
   },
   "outputs": [],
   "source": [
    "# Initialize data from WikiSQL\n",
    "train_data = list(load_dataset(\"wikisql\")[\"train\"])\n",
    "\n",
    "def createTrainExample(query):\n",
    "    table = query[\"table\"]\n",
    "    rows = [\n",
    "        {h: row[i] for (i, h) in enumerate(table[\"header\"])}\n",
    "        for row in table[\"rows\"]\n",
    "    ]\n",
    "    meta = \"\\n\".join(f'\"{h}\": {[row[h] for row in rows[:10]]}' for h in table[\"header\"])\n",
    "    prompt = f\"\"\"\n",
    "    Print a SQL query (over a table named \"table\" quoted with double quotes) that answers the question below.\n",
    "\n",
    "    You have the following columns:\n",
    "    {meta}\n",
    "\n",
    "    The format should be\n",
    "    Question: the question to ask\n",
    "    SQL: the SQL to generate\n",
    "\n",
    "    Question: {query['question']}\n",
    "    SQL: \"\"\".format()\n",
    "    example = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "            \"role\":\"system\",\n",
    "            \"content\":\"You are an expert at generating SQL. Respond with just SQL.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\":\"assistant\", \"content\": codegen_query(query)}\n",
    "        ]\n",
    "    }\n",
    "    print(example)\n",
    "    return example\n",
    "\n",
    "\n",
    "createTrainExample(train_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training examples for Google's Text Bison model\n",
    "def createTrainExampleInstruct(query):\n",
    "    table = query[\"table\"]\n",
    "    rows = [\n",
    "        {h: row[i] for (i, h) in enumerate(table[\"header\"])}\n",
    "        for row in table[\"rows\"]\n",
    "    ]\n",
    "    meta = \"\\n\".join(f'\"{h}\": {[row[h] for row in rows[:10]]}' for h in table[\"header\"])\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert at generating SQL. Respond with just SQL.\n",
    "\n",
    "    Print a SQL query (over a table named \"table\" quoted with double quotes) that answers the question below.\n",
    "\n",
    "    You have the following columns:\n",
    "    {meta}\n",
    "\n",
    "    The format should be\n",
    "    Question: the question to ask\n",
    "    SQL: the SQL to generate\n",
    "\n",
    "    Question: {query['question']}\n",
    "    SQL: \"\"\".format()\n",
    "    example = {\n",
    "        \"text_input\": prompt,\n",
    "        \"output\": codegen_query(query),\n",
    "    }\n",
    "    print(example)\n",
    "    return example\n",
    "\n",
    "\n",
    "createTrainExampleInstruct(train_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnESIJDOyXMG"
   },
   "outputs": [],
   "source": [
    "# Save our training data to a file\n",
    "with open('train-sql.JSONL', mode='w', newline='') as file:\n",
    "    for i in range(100):\n",
    "        row = createTrainExample(train_data[i])\n",
    "        #add each row\n",
    "        json.dump(row, file)\n",
    "        file.write('\\n')\n",
    "\n",
    "# Save our training data to a csv file\n",
    "\n",
    "import csv\n",
    "with open('train-sql.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['text_input', 'output'])\n",
    "    for i in range(100):\n",
    "        row = createTrainExampleInstruct(train_data[i])\n",
    "        #add each row\n",
    "        writer.writerow([row['text_input'], row['output']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune GPT3.5 Turbo for SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can optionally set if you finetuned through the web UI or in a separate tutorial.\n",
    "# If you set this, you can skip to the next section.\n",
    "# FTGPT=\"ft:...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkpWfJaKyXMG"
   },
   "outputs": [],
   "source": [
    "# Upload our training data to OpenAI\n",
    "file = openai.File.create(\n",
    "  file=open(\"train-sql.JSONL\", \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WZJ8NaayXMG"
   },
   "source": [
    "Wait 30s to 1 minute for the file to be processed. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgcYJc_uyXMG"
   },
   "outputs": [],
   "source": [
    "# Start a fine-tuning job\n",
    "job = openai.FineTuningJob.create(training_file=file['id'], model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r381T2YuyXMG"
   },
   "outputs": [],
   "source": [
    "# Wait for the fine-tune to complete\n",
    "FTGPT = \"\"\n",
    "for i in range(100):\n",
    "    check = openai.FineTuningJob.retrieve(job['id'])\n",
    "    if (check.fine_tuned_model):\n",
    "        FTGPT = check.fine_tuned_model\n",
    "        break\n",
    "    time.sleep(30)\n",
    "print(FTGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune text-bison for SQL\n",
    "\n",
    "Go to [Google's Makersuite and fine tune a text-bison model](https://makersuite.google.com/app/tuned_models/new_tuned_model) using the `train-sql.csv` file we generated above. Then, follow their [guide to authenticate](https://developers.generativeai.google/tutorials/oauth_quickstart) and install gcloud locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the client_scret.json in the same directory as this notebook file\n",
    "# This will open a browser link to authenticate with Google and create a file called \"application_default_credentials.json\" somewhere.\n",
    "!gcloud auth application-default login --client-id-file client_secret.json --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.tuning'\n",
    "\n",
    "# Copy the credentials file \"application_default_credentials.json\" to the same directory as this notebook file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import google.oauth2.credentials\n",
    "\n",
    "# Load the client secrets\n",
    "with open('client_secret.json', 'r') as f:\n",
    "    client_secrets = json.load(f)\n",
    "\n",
    "\n",
    "with open('application_default_credentials.json', 'r') as f:\n",
    "    token_info = json.load(f)\n",
    "\n",
    "    \n",
    "creds = google.oauth2.credentials.Credentials(\n",
    "    token=None,\n",
    "    refresh_token=token_info.get('refresh_token'),\n",
    "    token_uri=client_secrets['installed']['token_uri'],\n",
    "    client_id=client_secrets['installed']['client_id'],\n",
    "    client_secret=client_secrets['installed']['client_secret'],\n",
    ")\n",
    "\n",
    "palm.configure(credentials=creds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm.configure(credentials = creds)\n",
    "import google.generativeai as palm\n",
    "\n",
    "print('Available base models:', [m.name for m in palm.list_models()])\n",
    "print('My tuned models:', [m.name for m in palm.list_tuned_models()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googleFTModels = [m.name for m in palm.list_tuned_models()]\n",
    "FTBISON = googleFTModels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tE_1TUdyXMG"
   },
   "source": [
    "## 4. Evaluate our finetuned model\n",
    "\n",
    "Finally, we'll load in an evaluation dataset, define an evaluation function, and then compare our results with BrainTrust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9hb2mkuyXMG"
   },
   "outputs": [],
   "source": [
    "# load in an evaluation dataset\n",
    "data = list(load_dataset(\"wikisql\")[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def googleai_req(model, messages, max_tokens):\n",
    "    # only use messages for now\n",
    "    prompt = \"\\n\".join([m[\"content\"] for m in messages])\n",
    "    response = palm.generate_text(prompt=prompt, model=model )\n",
    "    result = response.result\n",
    "\n",
    "    return prompt, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgRmkBSYyXMH"
   },
   "outputs": [],
   "source": [
    "# Define a generation function\n",
    "def text2sql(query, modelName, type=\"openai\"):\n",
    "    table = query[\"table\"]\n",
    "    meta = \"\\n\".join(f'\"{h}\"' for h in table[\"header\"])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Print a SQL query (over a table named \"table\" quoted with double quotes) that answers the question below.\n",
    "USE THE DOUBLE QUOTES ON TABLE!\n",
    "You have the following columns:\n",
    "{meta}\n",
    "\n",
    "The format should be\n",
    "Question: the question to ask\n",
    "SQL: the SQL to generate\n",
    "\n",
    "Question: {query['question']}\n",
    "SQL: \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\":\"You are an expert at generating SQL. Respond with just SQL.\"\n",
    "         },\n",
    "         {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":prompt,\n",
    "         }\n",
    "    ]\n",
    "    print(\"RUNNING WITH: model:\", modelName)\n",
    "\n",
    "    if type == \"openai\":\n",
    "        prompt, resp = openai_req(model=modelName, messages=messages, max_tokens=1024)\n",
    "    if type == \"google\":\n",
    "        prompt, resp = googleai_req(model=modelName, messages=messages, max_tokens=1024)\n",
    "    print(resp)\n",
    "\n",
    "    return (\n",
    "        prompt,\n",
    "        resp,\n",
    "        resp.rstrip(\";\")\n",
    "        if resp\n",
    "        else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, resp, _ = text2sql(data[0], \"models/text-bison-001\", type=\"google\")\n",
    "print(prompt + green(resp))\n",
    "\n",
    "output_sql = resp.rstrip(\";\")\n",
    "table = get_table(data[0]['table'])\n",
    "print(\"Correct answer:\", data[0][\"sql\"][\"human_readable\"],)\n",
    "duckdb.arrow(table).query(\"table\", output_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPiKDn9UyXMH"
   },
   "outputs": [],
   "source": [
    "# Define an evaluation function\n",
    "def runEvaluation(modelName, type=\"openai\"):\n",
    "    # Initialize BrainTrust experiment\n",
    "    with braintrust.init(project=\"openai-google-battle-sql\", experiment=modelName) as experiment:\n",
    "        for i in range(NUM_TEST_EXAMPLES):\n",
    "            print(f\"{i+1}/{NUM_TEST_EXAMPLES}\")\n",
    "            query = data[i]\n",
    "            gt_query = codegen_query(query)\n",
    "            gt_answer = run_query(gt_query, query[\"table\"])\n",
    "        \n",
    "            prompt, _, sql = text2sql(query, modelName)\n",
    "            # Why?\n",
    "            sql = sql.replace(\"output:\", \"\")\n",
    "            try:\n",
    "                answer = run_query(sql, query[\"table\"])\n",
    "            except Exception as e:\n",
    "                answer = f\"FAILED: {e}\"\n",
    "        \n",
    "            #Log to BrainTrust\n",
    "            experiment.log(\n",
    "                input={\"question\": query[\"question\"]},\n",
    "                output=answer,\n",
    "                expected=gt_answer,\n",
    "                scores={\n",
    "                    \"answer\": score(gt_answer, answer),\n",
    "                    \"query\": score(gt_query, sql),\n",
    "                },\n",
    "                metadata={\n",
    "                    \"prompt\": prompt,\n",
    "                    \"gt_sql\": gt_query,\n",
    "                    \"output_sql\": sql,\n",
    "                    \"id\": i,\n",
    "                },\n",
    "            )\n",
    "        \n",
    "        # Print experiment results\n",
    "        print(experiment.summarize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwiF9KQEyXMH"
   },
   "outputs": [],
   "source": [
    "#Evaluate base text bison\n",
    "runEvaluation(\"models/text-bison-001\", type=\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate finetuned bison\n",
    "runEvaluation(FTBISON, type=\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the base 3.5 turbo model\n",
    "runEvaluation(\"gpt-3.5-turbo\", type=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z25N0KKuyXMH"
   },
   "outputs": [],
   "source": [
    "# Evaluate the fine tuned GPT 3.5 turbo model\n",
    "runEvaluation(FTGPT, type=\"openai\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
