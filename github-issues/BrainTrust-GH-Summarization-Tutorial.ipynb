{"cells":[{"cell_type":"markdown","metadata":{"id":"vigvBmpNxHvb"},"source":["# BrainTrust Summarization Tutorial (GitHub Issues)\n","\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/braintrustdata/braintrust-examples/blob/main/github-issues/BrainTrust-GH-Summarization-Tutorial.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\n","Welcome to [BrainTrust](https://www.braintrustdata.com/)! This tutorial will teach you the basics of working with BrainTrust to evaluate a text summarization use case, including creating a project, running experiments, and analyzing their results.\n","\n","In this notebook, we'll build an application that suggests better titles for issues in a GitHub repo, using their page content. We'll use a technique called **model graded evaluation** to automatically evaluate these titles against the original titles, and improve our prompt based on what we find.\n","\n","Before starting, please make sure that you have a BrainTrust account. If you do not, please [sign up](https://www.braintrustdata.com) or [get in touch](mailto:info@braintrustdata.com). After this tutorial, feel free to dig deeper by visiting [the docs](http://www.braintrustdata.com/docs)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5AFngU_sw1K5"},"outputs":[],"source":["# NOTE: Replace YOUR_OPENAI_API_KEY with your OpenAI API Key and YOUR_BRAINTRUST_API_KEY with your BrainTrust API key. Do not put it in quotes.\n","%env OPENAI_API_KEY=YOUR_OPENAI_API_KEY\n","%env BRAINTRUST_API_KEY=sk-YOUR_BRAINTRUST_API_KEY"]},{"cell_type":"markdown","metadata":{"id":"UBcGWTNO6WE7"},"source":["By default, this notebook uses the [Supabase](https://github.com/supabase/supabase) repository. You can tweak these `GITHUB_` environment variables below if you'd like to use a different one (including your own!)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UumnW1VOw1K6"},"outputs":[],"source":["# Optional: feel free to tweak this to a repo of your choice! If you set it to\n","# a new repo, then make sure to set the GITHUB_PERSONAL_ACCESS_TOKEN\n","%env GITHUB_REPO=supabase/supabase\n","%env GITHUB_PERSONAL_ACCESS_TOKEN="]},{"cell_type":"markdown","metadata":{"id":"c0hvsPRZLCUz"},"source":["We'll start by installing some dependencies, setting up the environment, and downloading the GitHub issues."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rTIA4DVQw1K7"},"outputs":[],"source":["%pip install autoevals braintrust guidance langchain requests openai tiktoken"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UDe2_sAw1K7"},"outputs":[],"source":["import guidance\n","import gzip\n","import json\n","import os\n","from pathlib import Path\n","import requests\n","import tiktoken\n","\n","\n","REPO = os.environ[\"GITHUB_REPO\"]\n","\n","MODEL = \"gpt-3.5-turbo\"\n","guidance.llm = guidance.llms.OpenAI(MODEL)\n","\n","\n","CACHE_PATH = Path(\"cache\")\n","os.makedirs(CACHE_PATH, exist_ok=True)\n","repo_fname = REPO.replace(\"/\", \"-\") + \".json\"\n","repo_cache = CACHE_PATH / repo_fname\n","repo_url = f\"https://braintrust-public.s3.amazonaws.com/{repo_fname}.gz\"\n","if repo_cache.exists():\n","    with open(repo_cache, \"r\") as f:\n","        issues = [json.loads(l) for l in f]\n","elif requests.head(repo_url).ok:\n","    with open(repo_cache, \"wb\") as f:\n","        f.write(gzip.decompress(requests.get(repo_url).content))\n","    with open(repo_cache, \"r\") as f:\n","        issues = [json.loads(l) for l in f]\n","elif \"GITHUB_PERSONAL_ACCESS_TOKEN\" in os.environ:\n","    # Use langchain to load the issues if they do not already exist\n","    # NOTE: This loader appears to get the issue, and its summary, but not the comments.\n","    from langchain.document_loaders import GitHubIssuesLoader\n","    loader = GitHubIssuesLoader(repo=REPO)\n","    issues = [d.__dict__ for d in loader.load()]\n","    with open(repo_cache, \"w\") as f:\n","        for d in issues:\n","            print(json.dumps(d), file=f)\n","else:\n","    raise Exception(\"Please set GITHUB_PERSONAL_ACCESS_TOKEN to explore a new repo\")\n","\n","print(f\"Loaded {len(issues)} issues from {REPO}\")\n","\n","N_ISSUES = 20\n","issues.sort(key=lambda d: d[\"metadata\"][\"created_at\"])\n","\n","# Remove PRs. Technically you can do this in the \"loader\", but we might as well download them so we can play with them later\n","issues = [d for d in issues if not d[\"metadata\"][\"is_pull_request\"]]\n","issues = issues[:N_ISSUES]\n","\n","# This notebook currently does not split/chunk/etc. the documents, so we need to make sure they are not too long\n","tokenizer = tiktoken.encoding_for_model(MODEL)\n","assert max(len(tokenizer.encode(d[\"page_content\"])) for d in issues) < 3000, max(len(tokenizer.encode(d[\"page_content\"])) for d in issues)"]},{"cell_type":"markdown","metadata":{"id":"3z5e5UG3w1K8"},"source":["## Writing the initial prompts\n","\n","Let's analyze the first example, and build up a prompt for generating a new title. We're using a library called [Guidance](https://github.com/microsoft/guidance), which makes it easy to template prompts and cache results. With BrainTrust, can use any library you'd like -- Guidance, LangChain, or even just direct calls to an LLM.\n","\n","The prompt provides the issue's description to the model, and asks it to generate a title. Note that it does _not_ provide the original title."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9x5dPZiIw1K8"},"outputs":[],"source":["issue = issues[0]\n","\n","print(issue[\"metadata\"][\"url\"], \"\\n\")\n","print(issue[\"metadata\"][\"title\"])\n","print(\"-\"*len(issue[\"metadata\"][\"title\"]))\n","print(issue[\"page_content\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yK7ZSQnw1K8"},"outputs":[],"source":["create_title = guidance('''\n","{{#system~}}\n","You are a technical project manager who helps software engineers generate better titles for their GitHub issues,\n","by looking at their issue descriptions. The titles should be clear and concise one-line statements.\n","{{~/system}}\n","\n","{{#user~}}\n","GitHub issue: {{input}}\n","{{~/user}}\n","\n","{{#assistant~}}\n","{{gen 'title' max_tokens=500}}\n","{{~/assistant}}''', async_mode=True)\n","\n","out = await create_title(input=issue[\"page_content\"])"]},{"cell_type":"markdown","metadata":{"id":"QaCmi5dhw1K9"},"source":["## Grading the new title\n","\n","Ok cool! The new title looks pretty good. But how do we consistently and automatically evaluate whether the new titles are better than the old ones?\n","\n","With subjective problems, like summarization, one great technique is to use an LLM to grade the outputs. This is known as model graded evaluation. Below, we'll use a [summarization prompt](https://github.com/braintrustdata/autoevals/blob/main/templates/summary.yaml)\n","from Braintrust's open source [autoevals](https://github.com/braintrustdata/autoevals) library. We encourage you to use these prompts, but also to copy/paste them, modify them, and create your own!\n","\n","The prompt uses [Chain of Thought](https://arxiv.org/abs/2201.11903) which dramatically improves a model's performance on grading tasks. Later, we'll see how it helps us debug the model's outputs too."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from autoevals import Summary\n","scorer = Summary()\n","\n","score = scorer(input=issue[\"page_content\"], output=out[\"title\"], expected=issue[\"metadata\"][\"title\"])\n","print(score.metadata['rationale'])\n","print(f\"Score={score.score}\")"]},{"cell_type":"markdown","metadata":{"id":"bAQ3gHsdh3JU"},"source":["## Running across the dataset\n","\n","Now that we have automated the process of generating titles and grading them, we can test the full set of `N_ISSUES` issues. This block uses the `Eval` framework in Braintrust to run the evaluation.\n","\n","Braintrust will automatically use Python's asynchronous runtime to evaluate the examples in parallel to optimize performance. Once the experiment is run, we can view the results in the Braintrust UI."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXzhZ8fdw1K9"},"outputs":[],"source":["from braintrust import Eval\n","\n","data = [\n","    {\"input\": issue[\"page_content\"], \"expected\": issue[\"metadata\"][\"title\"], \"metadata\": {\"doc\": issue[\"metadata\"]}}\n","    for issue in issues\n","]\n","\n","async def wrap_create_title(input, hooks):\n","    out = await create_title(input=input)\n","    return out[\"title\"]\n","\n","await Eval(\n","    \"gh-issues\",\n","    data = data,\n","    task=wrap_create_title,\n","    scores=[Summary],\n",")"]},{"cell_type":"markdown","metadata":{"id":"90TGWYWihDP7"},"source":["## Pause and analyze the results in BrainTrust!\n","\n","The cell above will print a link to the BrainTrust experiment. Go check it out (NOTE: it may take up to a minute to synchronize the data for viewing).\n","\n","Click around, and specifically look at the `Rationale` field for some of the records with a `summary` score of 0. Can you spot a trend? Look at the task with `metadata.number` 4833. We're going to explore it below,and see if we can improve the prompt for it and similar examples."]},{"cell_type":"markdown","metadata":{"id":"y0GuOVu-kFSM"},"source":["## Reproducing an example\n","\n","Now, let's pull down issue #4833 and reproduce the evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFZg3G0Yw1K9"},"outputs":[],"source":["issue = [x for x in issues if x[\"metadata\"][\"number\"] == 4833][0]\n","\n","print(issue[\"metadata\"][\"url\"], \"\\n\")\n","print(issue[\"metadata\"][\"title\"])\n","print(issue[\"page_content\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QIJsLAC-w1K-"},"outputs":[],"source":["score = scorer(input=issue[\"page_content\"], output=out[\"title\"], expected=issue[\"metadata\"][\"title\"])\n","print(score.metadata['rationale'])\n","print(f\"Score={score.score}\")"]},{"cell_type":"markdown","metadata":{"id":"kQ7bY4ATlDz5"},"source":["### Fixing the prompt\n","\n","Have you spotted the issue yet? It seems that some of our new titles are missing key details, and perhaps optimizing for brevity over accuracy. Let's tweak the prompt and see if we can improve. Note the last sentence in this prompt: _\"Make sure the title is accurate and comprehensive, over being concise.\"_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDdoPnYzw1K-"},"outputs":[],"source":["create_title = guidance('''\n","{{#system~}}\n","You are a technical project manager who helps software engineers generate better titles for their GitHub issues,\n","by looking at their issue descriptions. Make sure the title is accurate and comprehensive, over being concise.\n","{{~/system}}\n","\n","{{#user~}}\n","GitHub issue: {{input}}\n","{{~/user}}\n","\n","{{#assistant~}}\n","{{gen 'title' max_tokens=500}}\n","{{~/assistant}}''')\n","\n","out = create_title(input=issue[\"page_content\"])\n","\n","score = scorer(input=issue[\"page_content\"], output=out[\"title\"], expected=issue[\"metadata\"][\"title\"])\n","print(score.metadata['rationale'])\n","print(f\"Score={score.score}\")"]},{"cell_type":"markdown","metadata":{"id":"Epffw9yQlcw4"},"source":["## Assessing the change\n","\n","Awesome! The new method picked the generated title. But how do we know how it affects the overall dataset? Let's run the new prompt on our full set of issues, and take a look at the experiment.\n","\n","Once this finishes, we'll get a new link that allows us to compare the two experiments. Let's take a look at the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Epz2Wshw1K-"},"outputs":[],"source":["await Eval(\n","    \"gh-issues\",\n","    data = data,\n","    task=wrap_create_title,\n","    scores=[Summary],\n",")"]},{"cell_type":"markdown","metadata":{"id":"QStOc96ElyRR"},"source":["## Summary\n","\n","Click into the new experiment, and check it out! You should notice a few things:\n","\n","* BrainTrust will automatically compare the new experiment to your previous one.\n","* You should see an increase in scores, and can click around to look at exactly which values improved.\n","* You can also filter down to the examples who still have a `summary` score of 0, and further iterate on the prompt to try and improve them.\n","\n","Last but not least... This tutorial is designed to teach you about BrainTrust, but in practice, you probably will have to iterate with more than just one prompt change to improve results. In fact, I changed the prompt 4 or 5 times and iterated with BrainTrust before nailing down this solution.\n","\n","Happy evaling!"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"gh-issues-bot-nFR94B64-py3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
