{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"vigvBmpNxHvb"},"source":["# BrainTrust Summarization Tutorial (GitHub Issues)\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/braintrustdata/braintrust-examples/blob/main/github-issues/py/BrainTrust-GH-Summarization-Tutorial.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\n","Welcome to [BrainTrust](https://www.braintrustdata.com/)! This tutorial will teach you the basics of working with BrainTrust to evaluate a text summarization use case, including creating a project, running experiments, and analyzing their results.\n","\n","In this notebook, we'll build an application that suggests better titles for issues in a GitHub repo, using their page content. We'll use a technique called **model graded evaluation** to automatically evaluate these titles against the original titles, and improve our prompt based on what we find.\n","\n","Before starting, please make sure that you have a BrainTrust account. If you do not, please [sign up](https://www.braintrustdata.com) or [get in touch](mailto:info@braintrustdata.com). After this tutorial, feel free to dig deeper by visiting [the docs](http://www.braintrustdata.com/docs)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5AFngU_sw1K5"},"outputs":[],"source":["# NOTE: Replace YOUR_OPENAI_KEY with your OpenAI API Key and YOUR_BRAINTRUST_API_KEY with your BrainTrust API key. Do not put it in quotes.\n","%env OPENAI_API_KEY=YOUR_OPENAI_KEY\n","%env BRAINTRUST_API_KEY=YOUR_BRAINTRUST_API_KEY"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UBcGWTNO6WE7"},"source":["By default, this notebook uses the [Supabase](https://github.com/supabase/supabase) repository. You can tweak these `GITHUB_` environment variables below if you'd like to use a different one (including your own!)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UumnW1VOw1K6"},"outputs":[],"source":["# Optional: feel free to tweak this to a repo of your choice! If you set it to\n","# a new repo, then make sure to set the GITHUB_PERSONAL_ACCESS_TOKEN\n","%env GITHUB_REPO=supabase/supabase\n","%env GITHUB_PERSONAL_ACCESS_TOKEN="]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"c0hvsPRZLCUz"},"source":["We'll start by installing some dependencies, setting up the environment, and downloading the GitHub issues."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rTIA4DVQw1K7"},"outputs":[],"source":["%pip install braintrust guidance langchain requests openai tiktoken"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UDe2_sAw1K7"},"outputs":[],"source":["import asyncio\n","import braintrust\n","import concurrent.futures\n","import guidance\n","import gzip\n","import json\n","import os\n","from pathlib import Path\n","import re\n","import requests\n","import tiktoken\n","import time\n","\n","\n","REPO = os.environ[\"GITHUB_REPO\"]\n","\n","MODEL = \"gpt-3.5-turbo\"\n","guidance.llm = guidance.llms.OpenAI(MODEL)\n","\n","\n","CACHE_PATH = Path(\"cache\")\n","os.makedirs(CACHE_PATH, exist_ok=True)\n","repo_fname = REPO.replace(\"/\", \"-\") + \".json\"\n","repo_cache = CACHE_PATH / repo_fname\n","repo_url = f\"https://braintrust-public.s3.amazonaws.com/{repo_fname}.gz\"\n","if repo_cache.exists():\n","    with open(repo_cache, \"r\") as f:\n","        issues = [json.loads(l) for l in f]\n","elif requests.head(repo_url).ok:\n","    with open(repo_cache, \"wb\") as f:\n","        f.write(gzip.decompress(requests.get(repo_url).content))\n","    with open(repo_cache, \"r\") as f:\n","        issues = [json.loads(l) for l in f]\n","elif \"GITHUB_PERSONAL_ACCESS_TOKEN\" in os.environ:\n","    # Use langchain to load the issues if they do not already exist\n","    # NOTE: This loader appears to get the issue, and its summary, but not the comments.\n","    from langchain.document_loaders import GitHubIssuesLoader\n","    loader = GitHubIssuesLoader(repo=REPO)\n","    issues = [d.__dict__ for d in loader.load()]\n","    with open(repo_cache, \"w\") as f:\n","        for d in issues:\n","            print(json.dumps(d), file=f)\n","else:\n","    raise Exception(\"Please set GITHUB_PERSONAL_ACCESS_TOKEN to explore a new repo\")\n","\n","print(f\"Loaded {len(issues)} issues from {REPO}\")\n","\n","N_ISSUES = 20\n","issues.sort(key=lambda d: d[\"metadata\"][\"created_at\"])\n","\n","# Remove PRs. Technically you can do this in the \"loader\", but we might as well download them so we can play with them later\n","issues = [d for d in issues if not d[\"metadata\"][\"is_pull_request\"]]\n","issues = issues[:N_ISSUES]\n","\n","# This notebook currently does not split/chunk/etc. the documents, so we need to make sure they are not too long\n","tokenizer = tiktoken.encoding_for_model(MODEL)\n","assert max(len(tokenizer.encode(d[\"page_content\"])) for d in issues) < 3000, max(len(tokenizer.encode(d[\"page_content\"])) for d in issues)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3z5e5UG3w1K8"},"source":["## Writing the initial prompts\n","\n","Let's analyze the first example, and build up a prompt for generating a new title. We're using a library called [Guidance](https://github.com/microsoft/guidance), which makes it easy to template prompts and cache results. With BrainTrust, can use any library you'd like -- Guidance, LangChain, or even just direct calls to an LLM.\n","\n","The prompt provides the issue's description to the model, and asks it to generate a title. Note that it does _not_ provide the original title."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9x5dPZiIw1K8"},"outputs":[],"source":["issue = issues[0]\n","\n","print(issue[\"metadata\"][\"url\"], \"\\n\")\n","print(issue[\"metadata\"][\"title\"])\n","print(\"-\"*len(issue[\"metadata\"][\"title\"]))\n","print(issue[\"page_content\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yK7ZSQnw1K8"},"outputs":[],"source":["create_title = guidance('''\n","{{#system~}}\n","You are a technical project manager who helps software engineers generate better titles for their GitHub issues,\n","by looking at their issue descriptions. The titles should be clear and concise one-line statements.\n","{{~/system}}\n","\n","{{#user~}}\n","GitHub issue: {{page_content}}\n","{{~/user}}\n","\n","{{#assistant~}}\n","{{gen 'title' max_tokens=500}}\n","{{~/assistant}}''')\n","\n","out = create_title(page_content=issue[\"page_content\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QaCmi5dhw1K9"},"source":["## Grading the new title\n","\n","Ok cool! The new title looks pretty good. But how do we consistently and automatically evaluate whether the new titles are better than the old ones?\n","\n","With subjective problems, like summarization, one great technique is to use an LLM to grade the outputs. This is known as model graded evaluation. The prompt below provides the page content and both titles to the model (without specifying which is which), and asks it to describe their pros and cons, pick a superior title, and explain its thought process.\n","\n","Research like [Chain of Thought](https://arxiv.org/abs/2201.11903) shows that multi-step thinking dramatically improves a model's performance. Later, we'll see how it helps us debug the model's outputs too."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7iJeEioHw1K9"},"outputs":[],"source":["grade_title = guidance('''\n","{{#system~}}\n","You are a technical project manager who helps software engineers generate better titles for their GitHub issues.\n","You will look at the issue description, and pick which of two titles better describes it.\n","{{~/system}}\n","\n","{{#user~}}\n","I'm going to provide you with the issue description, and two possible titles.\n","\n","Issue Description: {{page_content}}\n","\n","Title 1: {{title1}}\n","Title 2: {{title2}}\n","\n","Please discuss each title briefly (one line for pros, one for cons), and then pick which one you think more accurately\n","summarizes the issue by writing \"Winner: 1\" or \"Winner: 2\", and then a short rationale for your choice.\n","{{~/user}}\n","\n","{{#assistant~}}\n","{{gen 'summary' max_tokens=500 temperature=0}}\n","{{~/assistant}}''')\n","\n","def parse_best_title(grade):\n","    return int(re.findall(\"Winner: (\\d+)\", grade[\"summary\"])[0])\n","\n","grade = grade_title(page_content=issue[\"page_content\"], title1=out[\"title\"], title2=issue[\"metadata\"][\"title\"])\n","print(parse_best_title(grade))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bAQ3gHsdh3JU"},"source":["## Running across the dataset\n","\n","Now that we have automated the process of generating titles and grading them, we can test the full set of `N_ISSUES` issues. This block uses Python's async features to generate and grade in parallel, effectively making your OpenAI account's rate limit the limiting factor.\n","\n","As it runs, it computes two different scores:\n","* `winner` is 1 if the model picks the generated title, or 0 if it picks the original\n","* `valid` is 1 if we're able to parse the model's graded output, and 0 otherwise\n","\n","Once this loop completes, you can view the results in BrainTrust."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXzhZ8fdw1K9"},"outputs":[],"source":["async def evaluate_issue(issue):\n","    title = await create_title(page_content=issue[\"page_content\"], async_mode=True)\n","    grade = await grade_title(page_content=issue[\"page_content\"], title1=title[\"title\"], title2=issue[\"metadata\"][\"title\"], async_mode=True)\n","    return (title, grade)\n","\n","async def run_on_all_issues():\n","    start = time.time()\n","    tasks = [asyncio.create_task(evaluate_issue(issue)) for issue in issues]\n","    title_grades = [await t for t in tasks]\n","    end = time.time()\n","    print(\"Took\", end - start, \"seconds\")\n","    return title_grades\n","\n","def analyze_experiment(title_grades):\n","    wins = {\"Model\": 0, \"Original\": 0}\n","    for doc, (title, grade) in zip(issues, title_grades):\n","        try:\n","            winner = \"Model\" if parse_best_title(grade) == 1 else \"Original\"\n","            wins[winner] += 1\n","        except IndexError:\n","            winner = \"Failed to parse\"\n","\n","        experiment.log(\n","            inputs={\"page_content\": doc[\"page_content\"]},\n","            output=title[\"title\"],\n","            expected=doc[\"metadata\"][\"title\"],\n","            metadata={\"doc\": doc[\"metadata\"], \"rationale\": grade[\"summary\"]},\n","            scores={\"winner\": 1 if winner == \"Model\" else 0, \"valid\": 1 if winner != \"Failed to parse\" else 0}\n","        )\n","\n","    print(experiment.summarize())\n","\n","# This line assumes there is an async event loop initialized but that the\n","# notebook is not running inside of an async task. This is true on Google Colab,\n","# but other notebook environments may vary. If you see errors while trying this\n","# code in a different tool, try changing this to `await run_on_all_issues()`, or\n","# initializing an event loop above with `asyncio.get_event_loop()`.\n","title_grades = asyncio.run(run_on_all_issues())\n","experiment = braintrust.init(project=\"gh-issue-titles\", experiment=\"original-prompt\")\n","analyze_experiment(title_grades)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"90TGWYWihDP7"},"source":["## Pause and analyze the results in BrainTrust!\n","\n","The cell above will print a link to the BrainTrust experiment. Go check it out (NOTE: it may take up to a minute to synchronize the data for viewing).\n","\n","Click around, and specifically look at the `Rationale` field for some of the records with a `winner` score of 0. Can you spot a trend? Look at the task with `metadata.number` 4833. We're going to explore it below, and see if we can improve the prompt for it and similar examples."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"y0GuOVu-kFSM"},"source":["## Reproducing an example\n","\n","Now, let's pull down issue #4833 and reproduce the evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFZg3G0Yw1K9"},"outputs":[],"source":["issue = [x for x in issues if x[\"metadata\"][\"number\"] == 4833][0]\n","\n","print(issue[\"metadata\"][\"url\"], \"\\n\")\n","print(issue[\"metadata\"][\"title\"])\n","print(issue[\"page_content\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QIJsLAC-w1K-"},"outputs":[],"source":["title_grade = asyncio.run(evaluate_issue(issue))\n","print(parse_best_title(title_grade[1]))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kQ7bY4ATlDz5"},"source":["### Fixing the prompt\n","\n","Have you spotted the issue yet? It seems that some of our new titles are missing key details, and perhaps optimizing for brevity over accuracy. Let's tweak the prompt and see if we can improve. Note the last sentence in this prompt: _\"Make sure the title is accurate and comprehensive, over being concise.\"_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDdoPnYzw1K-"},"outputs":[],"source":["create_title = guidance('''\n","{{#system~}}\n","You are a technical project manager who helps software engineers generate better titles for their GitHub issues,\n","by looking at their issue descriptions. Make sure the title is accurate and comprehensive, over being concise.\n","{{~/system}}\n","\n","{{#user~}}\n","GitHub issue: {{page_content}}\n","{{~/user}}\n","\n","{{#assistant~}}\n","{{gen 'title' max_tokens=500}}\n","{{~/assistant}}''')\n","\n","out = create_title(page_content=issue[\"page_content\"])\n","title_grade = await evaluate_issue(issue)\n","print(parse_best_title(title_grade[1]))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Epffw9yQlcw4"},"source":["## Assessing the change\n","\n","Awesome! The new method picked title 1, i.e. the generated title. But how do we know how it affects the overall dataset? Let's run the new prompt on our full set of issues, and take a look at the experiment.\n","\n","Once this finishes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Epz2Wshw1K-"},"outputs":[],"source":["title_grades = asyncio.run(run_on_all_issues())\n","experiment = braintrust.init(project=\"gh-issue-titles\", experiment=\"more-detailed-titles\")\n","analyze_experiment(title_grades)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QStOc96ElyRR"},"source":["## Summary\n","\n","Click into the new experiment, and check it out! You should notice a few things:\n","\n","* BrainTrust will automatically compare the new experiment to your previous one.\n","* You should see an increase in scores, and can click around to look at exactly which values improved.\n","* You can also filter down to the examples who still have a `winner` score of 0, and further iterate on the prompt to try and improve them.\n","\n","Last but not least... This tutorial is designed to teach you about BrainTrust, but in practice, you probably will have to iterate with more than just one prompt change to improve results. In fact, I changed the prompt 4 or 5 times and iterated with BrainTrust before nailing down this solution.\n","\n","Happy evaling!"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"gh-issues-bot-nFR94B64-py3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
