{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vigvBmpNxHvb"
   },
   "source": [
    "# BrainTrust Classification Tutorial (Article Titles)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/braintrustdata/braintrust-examples/blob/main/classify/BrainTrust-Classify-Tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Welcome to [BrainTrust](https://www.braintrustdata.com/)! This tutorial will teach you the basics of working with BrainTrust to evaluate a text classification use case, including creating a project, running experiments, and analyzing their results.\n",
    "\n",
    "Before starting, please make sure that you have a BrainTrust account. If you do not, please [sign up](https://www.braintrustdata.com) or [get in touch](mailto:info@braintrustdata.com). After this tutorial, feel free to dig deeper by visiting [the docs](http://www.braintrustdata.com/docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AFngU_sw1K5"
   },
   "outputs": [],
   "source": [
    "# NOTE: Replace YOUR_OPENAI_KEY with your OpenAI API Key and YOUR_BRAINTRUST_API_KEY with your BrainTrust API key. Do not put it in quotes.\n",
    "%env OPENAI_API_KEY=YOUR_OPENAI_KEY\n",
    "%env BRAINTRUST_API_KEY=YOUR_BRAINTRUST_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0hvsPRZLCUz"
   },
   "source": [
    "We'll start by installing some dependencies, setting up the environment, and downloading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTIA4DVQw1K7"
   },
   "outputs": [],
   "source": [
    "%pip install braintrust guidance openai datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UDe2_sAw1K7"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import braintrust\n",
    "import guidance\n",
    "import time\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "guidance.llm = guidance.llms.OpenAI(MODEL)\n",
    "\n",
    "# Load dataset from  Huggingface.\n",
    "dataset = load_dataset(\"ag_news\", split=\"train\")\n",
    "\n",
    "# Shuffle and trim to 20 datapoints. Restructure our dataset\n",
    "# slightly so that each item in the list contains the title\n",
    "# itself (\"text\") and the expected category index (\"label\").\n",
    "trimmed_dataset = dataset.shuffle()[:20]\n",
    "articles = [{\n",
    "    \"text\": trimmed_dataset[\"text\"][i],\n",
    "    \"label\": trimmed_dataset[\"label\"][i],\n",
    "    } for i in range(len(trimmed_dataset[\"text\"]))]\n",
    "\n",
    "# Extract category names from the dataset and build a map from index to\n",
    "# category name. We will use this to compare the expected categories to\n",
    "# those produced by the model.\n",
    "category_names = dataset.features['label'].names\n",
    "category_map = dict([i for i in enumerate(category_names)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z5e5UG3w1K8"
   },
   "source": [
    "## Writing the initial prompts\n",
    "\n",
    "Let's analyze the first example, and build up a prompt for categorizing a title. We're using a library called [Guidance](https://github.com/microsoft/guidance), which makes it easy to template prompts and cache results. With BrainTrust, can use any library you'd like -- Guidance, LangChain, or even just direct calls to an LLM.\n",
    "\n",
    "The prompt provides the article's title to the model, and asks it to generate a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x5dPZiIw1K8"
   },
   "outputs": [],
   "source": [
    "one_article = articles[0]\n",
    "print(one_article[\"text\"])\n",
    "print(one_article[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yK7ZSQnw1K8"
   },
   "outputs": [],
   "source": [
    "classify_article = guidance('''\n",
    "{{#system~}}\n",
    "You are an editor in a newspaper who helps writers identify the right category for their news articles,\n",
    "by reading the article's title. The category should be one of the following: World, Sports, Business\n",
    "or Sci-Tech. Reply with one word corresponding to the category.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "Article title: {{article_title}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'category' max_tokens=500}}\n",
    "{{~/assistant}}''')\n",
    "\n",
    "out = classify_article(article_title=one_article[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQ3gHsdh3JU"
   },
   "source": [
    "## Running across the dataset\n",
    "\n",
    "Now that we have automated the process of classifying titles, we can test the full set of articles. This block uses Python's async features to generate and grade in parallel, effectively making your OpenAI account's rate limit the limiting factor.\n",
    "\n",
    "As it runs, it compares the generated category to the expected one from the dataset. Once this loop completes, you can view the results in BrainTrust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXzhZ8fdw1K9"
   },
   "outputs": [],
   "source": [
    "async def evaluate_article(article):\n",
    "  full_output = classify_article(article_title=article[\"text\"])\n",
    "  category = full_output[\"category\"].strip().lower()\n",
    "  expected = category_map[article[\"label\"]].strip().lower()\n",
    "  return (article, full_output, expected, category)\n",
    "\n",
    "\n",
    "async def run_on_all_articles():\n",
    "    start = time.time()\n",
    "    tasks = [asyncio.create_task(evaluate_article(article)) for article in articles]\n",
    "    category_grades = [await t for t in tasks]\n",
    "    end = time.time()\n",
    "    print(\"Took\", end - start, \"seconds\")\n",
    "    return category_grades\n",
    "\n",
    "valid_categories = set(x.strip().lower() for x in category_map.values())\n",
    "def analyze_experiment(data):\n",
    "  for (article, full_output, expected, category) in categories:\n",
    "    experiment.log(\n",
    "        inputs={\"title\": article[\"text\"]},\n",
    "        output=category,\n",
    "        expected=expected,\n",
    "        scores={\n",
    "            \"match\": 1 if category == expected else 0,\n",
    "            \"valid\": 1 if category in valid_categories else 0,\n",
    "        },\n",
    "        metadata={\n",
    "           \"prompt\": str(full_output)\n",
    "        }\n",
    "    )\n",
    "\n",
    "  print(experiment.summarize())\n",
    "\n",
    "# This line assumes there is an async event loop initialized but that the\n",
    "# notebook is not running inside of an async task. This is true on Google Colab,\n",
    "# but other notebook environments may vary. If you see errors while trying this\n",
    "# code in a different tool, try changing this to `await run_on_all_articles()`, or\n",
    "# initializing an event loop above with `asyncio.get_event_loop()`.\n",
    "categories = asyncio.run(run_on_all_articles())\n",
    "experiment = braintrust.init(\n",
    "  project=\"classify-article-titles\",\n",
    "  experiment=\"original-prompt\"\n",
    ")\n",
    "analyze_experiment(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90TGWYWihDP7"
   },
   "source": [
    "## Pause and analyze the results in BrainTrust!\n",
    "\n",
    "The cell above will print a link to the BrainTrust experiment. Go check it out (NOTE: it may take up to a minute to synchronize the data for viewing).\n",
    "\n",
    "## Reproducing an example\n",
    "\n",
    "Now, let's pull down an issue corresponding to the \"Sci/Tech\" category and reproduce the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x5dPZiIw1K8"
   },
   "outputs": [],
   "source": [
    "sci_tech_category_index = category_names.index(\"Sci/Tech\")\n",
    "sci_tech_article = [a for a in articles if a[\"label\"] == sci_tech_category_index][0]\n",
    "print(sci_tech_article[\"text\"])\n",
    "print(sci_tech_article[\"label\"])\n",
    "\n",
    "out = classify_article(article_title=sci_tech_article[\"text\"])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQ3gHsdh3JU"
   },
   "source": [
    "### Fixing the prompt\n",
    "\n",
    "Have you spotted the issue yet? Looks like we have mispelled one of our categories in our prompt. The dataset's categories are \"World, Sports, Business and *Sci/Tech*\" - but we are using *Sci-Tech* in our prompt. Let's fix it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXzhZ8fdw1K9"
   },
   "outputs": [],
   "source": [
    "classify_article = guidance('''\n",
    "{{#system~}}\n",
    "You an editor in a newspaper who helps writers identify the right category for their news articles,\n",
    "by reading the article's title. The category should be one of the following: World, Sports, Business\n",
    "or Sci/Tech. Reply with one word corresponding to the category.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "Article title: {{article_title}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'category' max_tokens=500}}\n",
    "{{~/assistant}}''')\n",
    "\n",
    "out = classify_article(article_title=sci_tech_article[\"text\"])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQ3gHsdh3JU"
   },
   "source": [
    "### Assessing the change\n",
    "\n",
    "This time around, the model generated the expected categories. But how do we know how it affects the overall dataset? Let's run the new prompt on our full set of titles, and take a look at the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXzhZ8fdw1K9"
   },
   "outputs": [],
   "source": [
    "categories = asyncio.run(run_on_all_articles())\n",
    "experiment = braintrust.init(\n",
    "  project=\"classify-article-titles\",\n",
    "  experiment=\"fixed-categories\"\n",
    ")\n",
    "analyze_experiment(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQ3gHsdh3JU"
   },
   "source": [
    "## Summary\n",
    "\n",
    "Click into the new experiment, and check it out! You should notice a few things:\n",
    "\n",
    "* BrainTrust will automatically compare the new experiment to your previous one.\n",
    "* You should see an increase in scores, and can click around to look at exactly which values improved.\n",
    "* You can also filter down to the examples who still have a winner score of 1, and further iterate on the prompt to try and improve them.\n",
    "\n",
    "Last but not least... This tutorial is designed to teach you about BrainTrust, but in practice, you probably will have to iterate with more than just one prompt change to improve results.\n",
    "\n",
    "Happy evaling!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
