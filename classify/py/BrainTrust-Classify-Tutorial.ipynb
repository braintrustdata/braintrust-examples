{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vigvBmpNxHvb"
   },
   "source": [
    "# BrainTrust Classification Tutorial (Article Titles)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/braintrustdata/braintrust-examples/blob/main/classify/py/BrainTrust-Classify-Tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Welcome to [BrainTrust](https://www.braintrustdata.com/)! This is a quick tutorial on how to build and evaluate an AI app to classify news titles into categories.\n",
    "\n",
    "Before starting, make sure that you have a BrainTrust account. If you do not, please [sign up](https://www.braintrustdata.com) or [get in touch](mailto:info@braintrustdata.com). After this tutorial, learn more by visiting [the docs](http://www.braintrustdata.com/docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AFngU_sw1K5"
   },
   "outputs": [],
   "source": [
    "# NOTE: Replace YOUR_OPENAI_KEY with your OpenAI API Key and YOUR_BRAINTRUST_API_KEY with your BrainTrust API key. Do not put it in quotes.\n",
    "%env OPENAI_API_KEY=\n",
    "%env BRAINTRUST_API_KEY="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "c0hvsPRZLCUz"
   },
   "source": [
    "First, we'll install some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTIA4DVQw1K7"
   },
   "outputs": [],
   "source": [
    "%pip install braintrust openai datasets autoevals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the [ag_news dataset](https://huggingface.co/datasets/ag_news) from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UDe2_sAw1K7"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import braintrust\n",
    "import time\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from Huggingface.\n",
    "dataset = load_dataset(\"ag_news\", split=\"train\")\n",
    "\n",
    "# Shuffle and trim to 20 datapoints. Restructure our dataset\n",
    "# slightly so that each item in the list contains the title\n",
    "# itself (\"text\") and the expected category index (\"label\").\n",
    "trimmed_dataset = dataset.shuffle(seed=42)[:20]\n",
    "articles = [{\n",
    "    \"text\": trimmed_dataset[\"text\"][i],\n",
    "    \"label\": trimmed_dataset[\"label\"][i],\n",
    "    } for i in range(len(trimmed_dataset[\"text\"]))]\n",
    "\n",
    "# Extract category names from the dataset and build a map from index to\n",
    "# category name. We will use this to compare the expected categories to\n",
    "# those produced by the model.\n",
    "category_names = dataset.features['label'].names\n",
    "category_map = dict([name for name in enumerate(category_names)])\n",
    "valid_categories = set(x.strip().lower() for x in category_map.values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3z5e5UG3w1K8"
   },
   "source": [
    "## Writing the initial prompts\n",
    "\n",
    "Let's first write a prompt for categorizing a title for just one article. With BrainTrust, you can use any library you'd like -- Guidance, LangChain, or even just direct calls to an LLM.\n",
    "\n",
    "The prompt provides the article's title to the model, and asks it to generate a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x5dPZiIw1K8"
   },
   "outputs": [],
   "source": [
    "test_article = articles[0]\n",
    "test_label = test_article[\"label\"]\n",
    "test_text = test_article[\"text\"]\n",
    "\n",
    "print(\"Article Text:\",test_text)\n",
    "print(\"Article Label:\",test_label)\n",
    "print(\"Category:\", category_map[test_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yK7ZSQnw1K8"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "def classify_article(article_title):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are an editor in a newspaper who helps writers identify the right category for their news articles,\n",
    "by reading the article's title. The category should be one of the following: World, Sports, Business or Sci-Tech. Reply with one word corresponding to the category.\"\"\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Article title: {article_title} Category:\".format(article_title=article_title)\n",
    "        },]\n",
    "    result = openai.ChatCompletion.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=10,\n",
    "    )\n",
    "    category = result.choices[0].message.content\n",
    "    return (result, category)\n",
    "\n",
    "result, test_classify = classify_article(test_text)\n",
    "print(\"Classified as:\", test_classify)\n",
    "print(\"Score:\", 1 if test_classify == category_map[test_label] else 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQ3gHsdh3JU"
   },
   "source": [
    "## Running across the dataset\n",
    "\n",
    "Now that we have automated classifying titles, we can test the full set of articles. This block uses Python's async features to generate and grade in parallel.\n",
    "\n",
    "As it runs, we compare the generated category to the expected one from the dataset. Once this loop completes, you can view the results in BrainTrust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXzhZ8fdw1K9"
   },
   "outputs": [],
   "source": [
    "async def evaluate_article(article, classify_fn):\n",
    "  full_output, category = classify_fn(article_title=article[\"text\"])\n",
    "  category = category.strip().lower()\n",
    "  expected = category_map[article[\"label\"]].strip().lower()\n",
    "  return (article, full_output, expected, category)\n",
    "\n",
    "\n",
    "async def run_on_all_articles(classify_fn):\n",
    "    start = time.time()\n",
    "    tasks = [asyncio.create_task(evaluate_article(article, classify_fn)) for article in articles]\n",
    "    category_grades = [await t for t in tasks]\n",
    "    end = time.time()\n",
    "    print(\"Took\", end - start, \"seconds\")\n",
    "    return category_grades\n",
    "\n",
    "def analyze_experiment(categories, experiment):\n",
    "  for (article, full_output, expected, category) in categories:\n",
    "    experiment.log(\n",
    "        inputs={\"title\": article[\"text\"]},\n",
    "        output=category,\n",
    "        expected=expected,\n",
    "        scores={\n",
    "            \"match\": 1 if category == expected else 0,\n",
    "            \"valid\": 1 if category in valid_categories else 0,\n",
    "        },\n",
    "        metadata={\n",
    "           \"prompt\": full_output\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new experiment.\n",
    "experiment = braintrust.init(\n",
    "  project=\"classify-article-titles\",\n",
    "  experiment=\"original-prompt\"\n",
    ")\n",
    "\n",
    "categories = await run_on_all_articles(classify_article)\n",
    "\n",
    "analyze_experiment(categories, experiment)\n",
    "\n",
    "print(experiment.summarize())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "90TGWYWihDP7"
   },
   "source": [
    "## Pause and analyze the results in BrainTrust!\n",
    "\n",
    "The cell above will print a link to the BrainTrust experiment. Click on it to investigate where we can improve our AI app.\n",
    "\n",
    "\n",
    "Looking at our results table (in the screenshot below), we incorrectly output `sci-tech` instead of `sci/tech` which results in a failed eval test case. Let's fix it.\n",
    "\n",
    "![issue.png](issue.png)\n",
    "\n",
    "## Reproducing an example\n",
    "\n",
    "First, let's see if we can reproduce this issue locally. We can test an article corresponding to the \"Sci/Tech\" category and reproduce the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x5dPZiIw1K8"
   },
   "outputs": [],
   "source": [
    "sci_tech_article = [a for a in articles if \"Galaxy Clusters\" in a[\"text\"]][0]\n",
    "print(sci_tech_article[\"text\"])\n",
    "print(sci_tech_article[\"label\"])\n",
    "\n",
    "out = classify_article(article_title=sci_tech_article[\"text\"])\n",
    "print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQ3gHsdh3JU"
   },
   "source": [
    "### Fixing the prompt\n",
    "\n",
    "Have you spotted the issue? We have mispelled one of the categories in our prompt. The dataset's categories are \"World, Sports, Business and *Sci/Tech*\" - but we are using *Sci-Tech* in our prompt. Let's fix it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXzhZ8fdw1K9"
   },
   "outputs": [],
   "source": [
    "def classify_article_2(article_title):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are an editor in a newspaper who helps writers identify the right category for their news articles,\n",
    "by reading the article's title. The category should be one of the following: World, Sports, Business or Sci/Tech. Reply with one word corresponding to the category.\"\"\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Article title: {article_title} Category:\".format(article_title=article_title)\n",
    "        },]\n",
    "    result = openai.ChatCompletion.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=10,\n",
    "    )\n",
    "    category = result.choices[0].message.content\n",
    "    return (result, category)\n",
    "\n",
    "result, test_classify = classify_article_2(article_title=sci_tech_article[\"text\"])\n",
    "\n",
    "print(test_classify)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQ3gHsdh3JU"
   },
   "source": [
    "### Evaluate the new prompt\n",
    "\n",
    "The model classified the correct category `Sci/Tech` for this example. But, how do we know it works for the rest of the dataset? Let's run a new experiment to evaluate our new prompt using BrainTrust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXzhZ8fdw1K9"
   },
   "outputs": [],
   "source": [
    "# Initialize a new experiment.\n",
    "experiment = braintrust.init(\n",
    "  project=\"classify-article-titles\",\n",
    "  experiment=\"fixed-sci-tech-prompt\"\n",
    ")\n",
    "\n",
    "categories = await run_on_all_articles(classify_article_2)\n",
    "\n",
    "analyze_experiment(categories, experiment)\n",
    "\n",
    "print(experiment.summarize())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQ3gHsdh3JU"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Click into the new experiment, and check it out! You should notice a few things:\n",
    "\n",
    "![Compare](compare.png)\n",
    "\n",
    "* BrainTrust will automatically compare the new experiment to your previous one.\n",
    "* You should the eval scores increase and you can see which test cases improved.\n",
    "* You can also filter the test cases that have a low score and work on improving the prompt for those\n",
    "\n",
    "Now, you are on your journey of building reliable AI apps with BrainTrust.\n",
    "\n",
    "Learn more on our docs @ [https://www.braintrustdata.com/docs](https://www.braintrustdata.com/docs)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
