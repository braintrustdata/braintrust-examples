{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BrainTrust QA Chat Tutorial\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/braintrustdata/braintrust-examples/blob/main/\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Welcome to [BrainTrust](https://www.braintrustdata.com/)! This is a quick tutorial on how to build and evaluate an AI question and answer chat assistant. The assistant answers questions based on the user's information that is already saved in a vector DB (Chroma).\n",
    "\n",
    "Before starting, make sure that you have a BrainTrust account. If you do not, please [sign up](https://www.braintrustdata.com) or [get in touch](mailto:info@braintrustdata.com). After this tutorial, learn more by visiting [the docs](http://www.braintrustdata.com/docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import braintrust\n",
    "import chromadb\n",
    "import openai\n",
    "from autoevals.string import *\n",
    "from autoevals.llm import *\n",
    "\n",
    "PROJECTNAME = \"QAchatbot\"\n",
    "OPENAI_API_KEY = \"YOURAPIKEY\"\n",
    "BT_API_KEY=\"YOURAPIKEY\"\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in datasets\n",
    "\n",
    "First, we'll load two datasets:\n",
    "1. An evaluation dataset to test our pipeline. This includes input, output pairs like:\n",
    "```\n",
    "    {\"input\": \"What is my full name?\", \"output\": \"John Smith. -BT\"}\n",
    "```\n",
    "2. A user context dataset to give to the AI assistant as context. This will be stored in a vector db and contains rows like:\n",
    "```\n",
    "    {\"category\": \" address\", \"detail\": \"123 Main Street, Anytown, USA\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input, output pairs to evaluate our QAChatbot.\n",
    "# Upload these to a dataset in BrainTrust so your teammates can also easily use and manage this dataset.\n",
    "dataset = braintrust.init_dataset(PROJECTNAME, name=\"Basic Evaluation\", api_key=BT_API_KEY)\n",
    "with open('evaluationDataset.jsonl') as f:\n",
    "    for line in f:\n",
    "        testCase = json.loads(line)\n",
    "        dataset.insert(\n",
    "            input=testCase['input'],\n",
    "            output=testCase['output'],\n",
    "        )\n",
    "\n",
    "# List of facts about a fake user. These will be given as context to the QAChatbot.\n",
    "userContextDataset = []\n",
    "with open('userContextDataset.jsonl') as f:\n",
    "    for line in f:\n",
    "        userContextDataset.append(json.loads(line))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Store the user context dataset\n",
    "Next, we will embed the personal dataset of information into a vector DB. We will use [Chroma](https://www.trychroma.com/) to embed, store, and retrieve the relevant user context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"user-context\")\n",
    "\n",
    "for i,c in enumerate(userContextDataset):\n",
    "    fact = c[\"detail\"]\n",
    "    collection.add(\n",
    "        documents = [fact],\n",
    "        ids = [str(i)]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write an evaluation function\n",
    "\n",
    "Now, we will define a general evaluation function to test different prompts and pipelines using BrainTrust. This makes it easy to iterate and improve our AI app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is an evaluation script that tests a prompt and pipeline combination and logs the results to BrainTrust.\n",
    "def runEvaluation(generationFn, experimentName):\n",
    "    # Initialize a BrainTrust experiment\n",
    "    experiment = braintrust.init(project=PROJECTNAME, api_key=BT_API_KEY, experiment=experimentName, dataset=dataset)\n",
    " \n",
    "    for testCase in dataset:\n",
    "        input_data = str(testCase[\"input\"])\n",
    "        expected = str(testCase[\"output\"])\n",
    "    \n",
    "        output, prompt =  generationFn(input_data) # Generate an answer using the pipeline given\n",
    " \n",
    "        # Score factuality using BrainTrust's factuality scorer\n",
    "        # Learn more here: https://github.com/braintrustdata/autoevals/blob/main/templates/factuality.yaml\n",
    "        factuality = Factuality()\n",
    "        factualityScore = factuality(output, expected, input=input_data)\n",
    "\n",
    "        # Score the output using BrainTrust's Levenshtein scorer\n",
    "        levenEvaluator = LevenshteinScorer()\n",
    "        levenScore = levenEvaluator(output, expected, input=input_data)\n",
    "        \n",
    "        # Define two simple custom scorers to ensure the LLM is saying the right words\n",
    "        def BadWordScorer(output):\n",
    "            if \"ai language model\" in output.lower():\n",
    "                return 0 # Bad LLM :(\n",
    "            if \"sorry\" in output.lower():\n",
    "                return 0 # Bad LLM :(\n",
    "            if \"user\" in output.lower():\n",
    "                return 0 # Bad LLM :(\n",
    "            else:\n",
    "                return 1\n",
    "        \n",
    "        def GoodWordScorer(output):\n",
    "            if \"-BT\" in output:\n",
    "                return 1 # Good LLM :)\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        # Log the results to BrainTrust\n",
    "        experiment.log(\n",
    "            input_data,\n",
    "            output,\n",
    "            expected,\n",
    "            scores={\n",
    "                factualityScore.name: factualityScore.score,\n",
    "                levenScore.name: levenScore.score,\n",
    "                \"saysBadWords\": BadWordScorer(output),\n",
    "                \"saysThankYou\": GoodWordScorer(output)\n",
    "                \n",
    "            }, # Scores dictionary\n",
    "            metadata={\n",
    "                factualityScore.name: factualityScore.metadata,\n",
    "                \"prompt\": prompt,\n",
    "            }, # Metadata dictionary\n",
    "            dataset_record_id=testCase[\"id\"],\n",
    "        )\n",
    " \n",
    "    # Summarize the results\n",
    "    summary = experiment.summarize(summarize_scores=True)\n",
    "    return summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate and improve :)!\n",
    "\n",
    "Finally, we will test two different prompts and pipelines.\n",
    "\n",
    "Pipeline A uses a simple prompt and 1 relevant fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "=========================SUMMARY=========================\n",
      "pipelineA-001 compared to pipelineB-001:\n",
      "90.00% (-10.00%) 'saysThankYou' score\t(0 improvements, 4 regressions)\n",
      "48.50% (-00.50%) 'factuality'   score\t(7 improvements, 6 regressions)\n",
      "28.58% (-12.05%) 'levenshtein'  score\t(2 improvements, 15 regressions)\n",
      "85.00% (-10.00%) 'saysBadWords' score\t(0 improvements, 4 regressions)\n",
      "\n",
      "See results for all experiments in QAchatbot at https://www.braintrustdata.com/app/braintrustdata.com/p/QAchatbot\n",
      "See results for pipelineA-001 at https://www.braintrustdata.com/app/braintrustdata.com/p/QAchatbot/pipelineA-001\n"
     ]
    }
   ],
   "source": [
    "# 4. Run against different prompts and pipelines\n",
    "\n",
    "# A very simple pipeline that uses 1 fact as context\n",
    "def llmPipelineA(question):\n",
    "    # Get a relevant fact from the user context dataset\n",
    "    relevant = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=1\n",
    "    )\n",
    "    relevant_text = ','.join(relevant[\"documents\"][0])\n",
    "    prompt = \"\"\"\n",
    "        You are an assistant called BT. Help the user. Do not say you are an AI language model. Follow up with -BT.\n",
    "        Relevant information: {relevant}\n",
    "        Question: {question}\n",
    "        Answer:\n",
    "        \"\"\".format(question=question, relevant=relevant_text)\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "\n",
    "    result = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return result, prompt\n",
    "\n",
    "resultsA = runEvaluation(llmPipelineA, \"pipelineA\")\n",
    "print(resultsA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we run the block above, we can click on the link to the BrainTrust web UI to see how our pipeline performs.\n",
    "\n",
    "![web-ui.png](1.png)\n",
    "\n",
    "We seem to fail multiple test cases because our AI app apologizes and says \"I'm sorry\" too much. We can easily fix this with a prompt change. Let's also see if improving the number of relevant facts can improve our score.\n",
    "\n",
    "Let's define pipeline B below which uses 5 relevant facts as context and includes an updated prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "=========================SUMMARY=========================\n",
      "pipelineB-002 compared to pipelineA-001:\n",
      "46.50% (-00.50%) 'factuality'   score\t(5 improvements, 7 regressions)\n",
      "100.00% (+10.00%) 'saysThankYou' score\t(4 improvements, 0 regressions)\n",
      "40.24% (+11.18%) 'levenshtein'  score\t(14 improvements, 2 regressions)\n",
      "95.00% (+10.00%) 'saysBadWords' score\t(4 improvements, 0 regressions)\n",
      "\n",
      "See results for all experiments in QAchatbot at https://www.braintrustdata.com/app/braintrustdata.com/p/QAchatbot\n",
      "See results for pipelineB-002 at https://www.braintrustdata.com/app/braintrustdata.com/p/QAchatbot/pipelineB-002\n"
     ]
    }
   ],
   "source": [
    "# A pipeline that uses 5 facts as context\n",
    "def llmPipelineB(question):\n",
    "    # Get relevant facts from the user context dataset\n",
    "    relevant = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=5\n",
    "    )\n",
    "    relevant_text = ','.join(relevant[\"documents\"][0])\n",
    "    prompt = \"\"\"\n",
    "        You are a very helpful assistant called BT. Respond concisely. Do not say you are an AI language model and do not apologize. End your answer with -BT.\n",
    "        Relevant information: {relevant}\n",
    "        Question: {question}\n",
    "        Your answer:\n",
    "        \"\"\".format(question=question, relevant=relevant_text)\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "\n",
    "    result = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    return result, prompt\n",
    "\n",
    "resultsB = runEvaluation(llmPipelineB, \"pipelineB\")\n",
    "print(resultsB)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we run the block above, we can click on the link to the BrainTrust web UI to see how our pipeline performs.\n",
    "\n",
    "![web-ui-final.png](2.png)\n",
    "\n",
    "We can verify that our pipeline changes actually improved our performance! Next, you can continue to make prompt and pipeline changes to improve the score even more.\n",
    "\n",
    "Now, you are on your journey of building reliable AI apps with BrainTrust.\n",
    "\n",
    "Learn more on our docs @ [https://www.braintrustdata.com/docs](https://www.braintrustdata.com/docs)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
